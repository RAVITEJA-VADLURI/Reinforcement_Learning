{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdM/hJwyZdAM3VeHxfDq5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAVITEJA-VADLURI/Reinforcement_Learning/blob/main/2303A51942_ASGN(8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Policy Gradient Methods & the REINFORCE Algorithm\n",
        "\n",
        "Below is a compact, assignment-ready explanation **plus** a ready-to-run PyTorch implementation of the **REINFORCE (Monte Carlo policy gradient)** algorithm using the CartPole-v1 environment. Use the explanation for theory, and the code to demonstrate the algorithm experimentally.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. High-level introduction\n",
        "\n",
        "Policy gradient (PG) methods directly parameterize and optimize a policy ( \\pi_\\theta(a|s) ) to maximize expected return ( J(\\theta) = \\mathbb{E}*{\\tau\\sim\\pi*\\theta}[R(\\tau)] ).\n",
        "Unlike value-based methods (Q-learning, DQN), PG methods optimize over stochastic policies and are naturally suited to continuous action spaces and stochastic policies. They work by computing (or estimating) the gradient ( \\nabla_\\theta J(\\theta) ) and performing gradient ascent on ( \\theta ).\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Core concepts (concise)\n",
        "\n",
        "* **Policy** ( \\pi_\\theta(a|s) ): probability of action (a) in state (s) parameterized by (\\theta) (e.g., neural network weights).\n",
        "* **Trajectory** (\\tau = (s_0,a_0,r_1,s_1,a_1,r_2,\\dots)).\n",
        "* **Return** ( G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1} ).\n",
        "* **Performance objective** ( J(\\theta) = \\mathbb{E}*{\\tau\\sim\\pi*\\theta}[G_0] ).\n",
        "* **Policy gradient theorem** (intuitively):\n",
        "  [\n",
        "  \\nabla_\\theta J(\\theta) = \\mathbb{E}*{\\tau\\sim\\pi*\\theta}\\Big[ \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t),G_t \\Big],\n",
        "  ]\n",
        "  so we can use sampled trajectories and Monte Carlo estimates to compute the gradient without differentiating environment dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "# 3. REINFORCE algorithm (conceptual steps)\n",
        "\n",
        "1. Initialize policy parameters (\\theta).\n",
        "2. Repeat for many episodes:\n",
        "\n",
        "   * Generate a full episode trajectory ( \\tau ) by following (\\pi_\\theta).\n",
        "   * For each timestep (t) in the episode compute discounted return (G_t).\n",
        "   * For each timestep (t) compute gradient estimate: ( \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t ).\n",
        "   * Sum/average these gradients over the episode and update:\n",
        "     [\n",
        "     \\theta \\leftarrow \\theta + \\alpha \\sum_{t}\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t),G_t\n",
        "     ]\n",
        "     where (\\alpha) is the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Variance reduction (practical note)\n",
        "\n",
        "Direct REINFORCE has **high variance**. Common improvements:\n",
        "\n",
        "* **Baseline** (b(s)) subtraction: use advantage (A_t = G_t - b(s_t)). If (b(s)) is the state value (V_\\phi(s)) (learned), variance reduces while keeping the gradient unbiased.\n",
        "* Use **reward-to-go** (use (G_t) instead of full-episode return) — already typical.\n",
        "* Use **entropy bonus** to encourage exploration.\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Pseudocode (compact)\n",
        "\n",
        "```\n",
        "Initialize policy parameters θ\n",
        "for episode = 1..N:\n",
        "    generate trajectory τ by following πθ\n",
        "    compute G_t for each time step t in τ\n",
        "    for each timestep t:\n",
        "        θ ← θ + α * ∇θ log πθ(a_t|s_t) * (G_t - baseline(s_t))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 6. REINFORCE — PyTorch implementation (CartPole-v1)\n",
        "\n",
        "This is a minimal, clear implementation using a neural policy that outputs action probabilities. It uses reward-to-go and an optional baseline (value network) is *not* included here — see comments for adding a baseline."
      ],
      "metadata": {
        "id": "eWFArwzcQmpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reinforce_cartpole.py\n",
        "import numpy as np\n",
        "np.bool8 = np.bool_\n",
        "\n",
        "import gym\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# ---------------------\n",
        "# Hyperparameters\n",
        "# ---------------------\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "SEED = 1\n",
        "GAMMA = 0.99\n",
        "LR = 1e-3\n",
        "HIDDEN_SIZE = 128\n",
        "MAX_EPISODES = 1000\n",
        "PRINT_INTERVAL = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_BASELINE = False  # If True, implement a value network baseline (not implemented below)\n",
        "\n",
        "# ---------------------\n",
        "# Utilities for gym compatibility\n",
        "# ---------------------\n",
        "def safe_reset(env):\n",
        "    res = env.reset()\n",
        "    return res[0] if isinstance(res, tuple) else res\n",
        "\n",
        "def safe_step(env, action):\n",
        "    res = env.step(action)\n",
        "    if len(res) == 5:  # gym >=0.26 (obs, reward, terminated, truncated, info)\n",
        "        obs, reward, terminated, truncated, info = res\n",
        "        done = terminated or truncated\n",
        "        return obs, reward, done, info\n",
        "    obs, reward, done, info = res\n",
        "    return obs, reward, done, info\n",
        "\n",
        "def set_seed(env, seed=SEED):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    env.seed(seed) if hasattr(env, 'seed') else None\n",
        "    try:\n",
        "        env.action_space.seed(seed)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ---------------------\n",
        "# Policy Network\n",
        "# ---------------------\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim, hidden=HIDDEN_SIZE):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.net(x)\n",
        "        return torch.softmax(logits, dim=-1)\n",
        "\n",
        "# ---------------------\n",
        "# Helper: compute discounted rewards-to-go\n",
        "# ---------------------\n",
        "def compute_returns(rewards, gamma=GAMMA):\n",
        "    # reward-to-go: G_t = r_t + gamma*r_{t+1} + ...\n",
        "    returns = []\n",
        "    R = 0.0\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    return returns\n",
        "\n",
        "# ---------------------\n",
        "# Training loop\n",
        "# ---------------------\n",
        "def train():\n",
        "    env = gym.make(ENV_ID)\n",
        "    set_seed(env, SEED)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy = PolicyNet(obs_dim, action_dim).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(policy.parameters(), lr=LR)\n",
        "\n",
        "    running_rewards = []\n",
        "\n",
        "    for episode in range(1, MAX_EPISODES + 1):\n",
        "        obs = safe_reset(env)\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "\n",
        "        # generate one episode (full trajectory)\n",
        "        while not done:\n",
        "            obs_tensor = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
        "            probs = policy(obs_tensor).squeeze(0)\n",
        "            dist = Categorical(probs)\n",
        "            action = dist.sample().item()\n",
        "            log_prob = dist.log_prob(torch.tensor(action, device=DEVICE))\n",
        "            next_obs, reward, done, _ = safe_step(env, action)\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "        # compute returns\n",
        "        returns = compute_returns(rewards)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
        "        # optional: normalize returns for stability\n",
        "        returns = (returns - returns.mean()) / (returns.std(unbiased=False) + 1e-8)\n",
        "\n",
        "        # policy gradient step (REINFORCE)\n",
        "        policy_loss = []\n",
        "        for log_prob, G in zip(log_probs, returns):\n",
        "            policy_loss.append(-log_prob * G)\n",
        "        policy_loss = torch.stack(policy_loss).sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        episode_return = sum(rewards)\n",
        "        running_rewards.append(episode_return)\n",
        "\n",
        "        if episode % PRINT_INTERVAL == 0:\n",
        "            avg_return = np.mean(running_rewards[-PRINT_INTERVAL:])\n",
        "            print(f\"Episode {episode}\\tAvgReturn(last {PRINT_INTERVAL}) = {avg_return:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    train()\n",
        "    print(\"Elapsed:\", time.time() - start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1bt3wNGuRnmg",
        "outputId": "392644c8-383b-41c6-a850-7896acafbd4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10\tAvgReturn(last 10) = 22.60\n",
            "Episode 20\tAvgReturn(last 10) = 21.10\n",
            "Episode 30\tAvgReturn(last 10) = 32.90\n",
            "Episode 40\tAvgReturn(last 10) = 30.30\n",
            "Episode 50\tAvgReturn(last 10) = 33.00\n",
            "Episode 60\tAvgReturn(last 10) = 44.50\n",
            "Episode 70\tAvgReturn(last 10) = 41.50\n",
            "Episode 80\tAvgReturn(last 10) = 52.10\n",
            "Episode 90\tAvgReturn(last 10) = 52.30\n",
            "Episode 100\tAvgReturn(last 10) = 68.90\n",
            "Episode 110\tAvgReturn(last 10) = 168.70\n",
            "Episode 120\tAvgReturn(last 10) = 168.70\n",
            "Episode 130\tAvgReturn(last 10) = 205.10\n",
            "Episode 140\tAvgReturn(last 10) = 170.40\n",
            "Episode 150\tAvgReturn(last 10) = 220.40\n",
            "Episode 160\tAvgReturn(last 10) = 228.00\n",
            "Episode 170\tAvgReturn(last 10) = 210.50\n",
            "Episode 180\tAvgReturn(last 10) = 207.00\n",
            "Episode 190\tAvgReturn(last 10) = 175.50\n",
            "Episode 200\tAvgReturn(last 10) = 167.20\n",
            "Episode 210\tAvgReturn(last 10) = 203.90\n",
            "Episode 220\tAvgReturn(last 10) = 216.10\n",
            "Episode 230\tAvgReturn(last 10) = 354.60\n",
            "Episode 240\tAvgReturn(last 10) = 261.60\n",
            "Episode 250\tAvgReturn(last 10) = 362.20\n",
            "Episode 260\tAvgReturn(last 10) = 477.60\n",
            "Episode 270\tAvgReturn(last 10) = 311.50\n",
            "Episode 280\tAvgReturn(last 10) = 230.90\n",
            "Episode 290\tAvgReturn(last 10) = 239.30\n",
            "Episode 300\tAvgReturn(last 10) = 383.80\n",
            "Episode 310\tAvgReturn(last 10) = 481.00\n",
            "Episode 320\tAvgReturn(last 10) = 318.70\n",
            "Episode 330\tAvgReturn(last 10) = 210.10\n",
            "Episode 340\tAvgReturn(last 10) = 213.90\n",
            "Episode 350\tAvgReturn(last 10) = 271.20\n",
            "Episode 360\tAvgReturn(last 10) = 341.80\n",
            "Episode 370\tAvgReturn(last 10) = 329.30\n",
            "Episode 380\tAvgReturn(last 10) = 264.40\n",
            "Episode 390\tAvgReturn(last 10) = 242.90\n",
            "Episode 400\tAvgReturn(last 10) = 388.10\n",
            "Episode 410\tAvgReturn(last 10) = 427.70\n",
            "Episode 420\tAvgReturn(last 10) = 463.00\n",
            "Episode 430\tAvgReturn(last 10) = 473.60\n",
            "Episode 440\tAvgReturn(last 10) = 315.90\n",
            "Episode 450\tAvgReturn(last 10) = 252.90\n",
            "Episode 460\tAvgReturn(last 10) = 248.40\n",
            "Episode 470\tAvgReturn(last 10) = 467.20\n",
            "Episode 480\tAvgReturn(last 10) = 482.10\n",
            "Episode 490\tAvgReturn(last 10) = 500.00\n",
            "Episode 500\tAvgReturn(last 10) = 481.70\n",
            "Episode 510\tAvgReturn(last 10) = 500.00\n",
            "Episode 520\tAvgReturn(last 10) = 500.00\n",
            "Episode 530\tAvgReturn(last 10) = 455.40\n",
            "Episode 540\tAvgReturn(last 10) = 285.80\n",
            "Episode 550\tAvgReturn(last 10) = 258.40\n",
            "Episode 560\tAvgReturn(last 10) = 421.80\n",
            "Episode 570\tAvgReturn(last 10) = 475.30\n",
            "Episode 580\tAvgReturn(last 10) = 463.30\n",
            "Episode 590\tAvgReturn(last 10) = 491.30\n",
            "Episode 600\tAvgReturn(last 10) = 474.60\n",
            "Episode 610\tAvgReturn(last 10) = 445.40\n",
            "Episode 620\tAvgReturn(last 10) = 490.20\n",
            "Episode 630\tAvgReturn(last 10) = 500.00\n",
            "Episode 640\tAvgReturn(last 10) = 472.80\n",
            "Episode 650\tAvgReturn(last 10) = 478.00\n",
            "Episode 660\tAvgReturn(last 10) = 438.20\n",
            "Episode 670\tAvgReturn(last 10) = 360.90\n",
            "Episode 680\tAvgReturn(last 10) = 411.80\n",
            "Episode 690\tAvgReturn(last 10) = 406.40\n",
            "Episode 700\tAvgReturn(last 10) = 379.80\n",
            "Episode 710\tAvgReturn(last 10) = 492.50\n",
            "Episode 720\tAvgReturn(last 10) = 449.10\n",
            "Episode 730\tAvgReturn(last 10) = 360.60\n",
            "Episode 740\tAvgReturn(last 10) = 451.70\n",
            "Episode 750\tAvgReturn(last 10) = 500.00\n",
            "Episode 760\tAvgReturn(last 10) = 446.00\n",
            "Episode 770\tAvgReturn(last 10) = 399.00\n",
            "Episode 780\tAvgReturn(last 10) = 500.00\n",
            "Episode 790\tAvgReturn(last 10) = 500.00\n",
            "Episode 800\tAvgReturn(last 10) = 500.00\n",
            "Episode 810\tAvgReturn(last 10) = 500.00\n",
            "Episode 820\tAvgReturn(last 10) = 500.00\n",
            "Episode 830\tAvgReturn(last 10) = 500.00\n",
            "Episode 840\tAvgReturn(last 10) = 457.60\n",
            "Episode 850\tAvgReturn(last 10) = 500.00\n",
            "Episode 860\tAvgReturn(last 10) = 469.30\n",
            "Episode 870\tAvgReturn(last 10) = 401.60\n",
            "Episode 880\tAvgReturn(last 10) = 458.70\n",
            "Episode 890\tAvgReturn(last 10) = 457.70\n",
            "Episode 900\tAvgReturn(last 10) = 457.40\n",
            "Episode 910\tAvgReturn(last 10) = 376.30\n",
            "Episode 920\tAvgReturn(last 10) = 427.60\n",
            "Episode 930\tAvgReturn(last 10) = 171.70\n",
            "Episode 940\tAvgReturn(last 10) = 500.00\n",
            "Episode 950\tAvgReturn(last 10) = 500.00\n",
            "Episode 960\tAvgReturn(last 10) = 500.00\n",
            "Episode 970\tAvgReturn(last 10) = 444.50\n",
            "Episode 980\tAvgReturn(last 10) = 500.00\n",
            "Episode 990\tAvgReturn(last 10) = 500.00\n",
            "Episode 1000\tAvgReturn(last 10) = 500.00\n",
            "Training finished.\n",
            "Elapsed: 435.3313720226288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# 7. How to run, dependencies & notes\n",
        "\n",
        "* Install dependencies: `pip install torch gym numpy`\n",
        "  (If you prefer `gymnasium`, minor `reset/step` semantics differ — adjust `safe_reset` and `safe_step` accordingly.)\n",
        "* Run: `python reinforce_cartpole.py`\n",
        "* Expected behavior: REINFORCE is simple but high-variance. On CartPole you may see gradual improvement, but convergence is slower and noisier than actor-critic or PPO. Use more episodes, a baseline (value network), or advantage normalization for better performance.\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Extensions / improvements (quick list)\n",
        "\n",
        "* **Add baseline**: train a value network (V_\\phi(s)) and use advantage (A_t = G_t - V_\\phi(s_t)). This reduces variance.\n",
        "* **Actor-Critic**: update policy using estimated advantage from a critic trained by bootstrapping (TD).\n",
        "* **Entropy bonus**: add (-\\beta \\cdot \\mathbb{E}[\\log \\pi]) to the loss to keep exploration.\n",
        "* **Mini-batch & vectorized envs**: collect multiple episodes in parallel (faster and lower variance).\n",
        "* **Use reward normalization or advantage normalization** for more stable training.\n",
        "\n",
        "---\n",
        "\n",
        "# 9. Short assignment-ready summary (copy into your report)\n",
        "\n",
        "Policy gradient methods directly parameterize a stochastic policy and optimize its parameters by gradient ascent on expected return. REINFORCE is a straightforward Monte Carlo policy gradient method that estimates the policy gradient using complete sampled episodes. It computes gradients of log-probabilities weighted by the trajectory returns (G_t). REINFORCE is unbiased but high-variance; common practical improvements include subtracting a baseline (e.g., value function), advantage normalization, and using actor-critic architectures. A simple PyTorch implementation applied to CartPole demonstrates core ideas: parameterize a policy network, sample episodes, compute returns, and update parameters with (\\nabla_\\theta \\log \\pi_\\theta(a|s),G_t).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gm_S-h3tRqA2"
      }
    }
  ]
}