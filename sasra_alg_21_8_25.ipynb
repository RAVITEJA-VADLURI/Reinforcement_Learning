{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYNXVLbdMDq3hgOmQIiR8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAVITEJA-VADLURI/Reinforcement_Learning/blob/main/sasra_alg_21_8_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ptFWZuk6A0e"
      },
      "outputs": [],
      "source": [
        "# ==============================================\n",
        "# GridWorld + TD(0) + SARSA (Colab Ready)\n",
        "# ==============================================\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------\n",
        "# GridWorld Environment\n",
        "# -------------------------\n",
        "class GridWorld:\n",
        "    ACTIONS = [0, 1, 2, 3]   # 0:UP, 1:RIGHT, 2:DOWN, 3:LEFT\n",
        "    DIRS = {0:(-1,0), 1:(0,1), 2:(1,0), 3:(0,-1)}\n",
        "    ARROWS = {0:'↑', 1:'→', 2:'↓', 3:'←'}\n",
        "\n",
        "    def _init_(self, H=5, W=5, walls=None, terminals=None, step_cost=-0.04,\n",
        "                 start=None, stochastic_slip=0.0, seed=0):\n",
        "        self.H, self.W = H, W\n",
        "        self.walls = set(walls or [])\n",
        "        self.terminals = dict(terminals or {})\n",
        "        self.step_cost = step_cost\n",
        "        self.start = start\n",
        "        self.stochastic_slip = float(stochastic_slip)\n",
        "        self.rng = random.Random(seed)\n",
        "\n",
        "        self.states = [(r,c) for r in range(H) for c in range(W)\n",
        "                       if (r,c) not in self.walls]\n",
        "        self.reset()\n",
        "\n",
        "    def in_bounds(self, r, c):\n",
        "        return 0 <= r < self.H and 0 <= c < self.W\n",
        "\n",
        "    def is_terminal(self, s):\n",
        "        return s in self.terminals\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        if self.start is not None:\n",
        "            return self.start\n",
        "        choices = [s for s in self.states if s not in self.terminals]\n",
        "        return self.rng.choice(choices)\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = self.sample_initial_state()\n",
        "        return self.s\n",
        "\n",
        "    def step(self, a):\n",
        "        r, c = self.s\n",
        "        if self.is_terminal(self.s):\n",
        "            return self.s, 0.0, True\n",
        "\n",
        "        eff_a = a\n",
        "        if self.stochastic_slip > 0.0 and self.rng.random() < self.stochastic_slip:\n",
        "            eff_a = (a + self.rng.choice([1,3])) % 4\n",
        "\n",
        "        dr, dc = self.DIRS[eff_a]\n",
        "        nr, nc = r + dr, c + dc\n",
        "        if (not self.in_bounds(nr, nc)) or ((nr, nc) in self.walls):\n",
        "            nr, nc = r, c\n",
        "\n",
        "        ns = (nr, nc)\n",
        "        reward = self.step_cost\n",
        "        if ns in self.terminals:\n",
        "            reward += self.terminals[ns]\n",
        "        self.s = ns\n",
        "        done = self.is_terminal(ns)\n",
        "        return ns, reward, done\n",
        "\n",
        "    def all_states(self):\n",
        "        return list(self.states)\n",
        "\n",
        "    def legal_actions(self, s=None):\n",
        "        return list(self.ACTIONS)\n",
        "\n",
        "# -------------------------\n",
        "# Helper functions\n",
        "# -------------------------\n",
        "def epsilon_greedy(Q, s, actions, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    qs = np.array([Q[(s,a)] for a in actions])\n",
        "    max_q = qs.max()\n",
        "    best = [a for a in actions if Q[(s,a)] == max_q]\n",
        "    return random.choice(best)\n",
        "\n",
        "def greedy_policy_from_V(env, V):\n",
        "    policy = {}\n",
        "    for s in env.all_states():\n",
        "        if env.is_terminal(s):\n",
        "            policy[s] = None\n",
        "            continue\n",
        "        best_a, best_val = None, -1e9\n",
        "        for a in env.legal_actions(s):\n",
        "            (r,c) = s\n",
        "            dr, dc = env.DIRS[a]\n",
        "            nr, nc = r+dr, c+dc\n",
        "            if (not env.in_bounds(nr,nc)) or ((nr,nc) in env.walls):\n",
        "                ns = (r,c)\n",
        "            else:\n",
        "                ns = (nr,nc)\n",
        "            val = V[ns]\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_a = a\n",
        "        policy[s] = best_a\n",
        "    return policy\n",
        "\n",
        "def values_to_array(env, V):\n",
        "    A = np.zeros((env.H, env.W))\n",
        "    A[:] = np.nan\n",
        "    for s in env.all_states():\n",
        "        r,c = s\n",
        "        A[r,c] = V[s]\n",
        "    return A\n",
        "\n",
        "def plot_values(env, V, title=\"State-Value (V)\"):\n",
        "    A = values_to_array(env, V)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    im = plt.imshow(A, interpolation='nearest')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            if (r,c) in env.walls:\n",
        "                txt = '■'\n",
        "            elif (r,c) in env.terminals:\n",
        "                txt = f\"T\\n{env.terminals[(r,c)]:.1f}\"\n",
        "            else:\n",
        "                txt = f\"{A[r,c]:.2f}\"\n",
        "            plt.text(c, r, txt, ha='center', va='center', fontsize=9)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def print_policy(env, policy, title=\"Policy\"):\n",
        "    print(title)\n",
        "    for r in range(env.H):\n",
        "        row = []\n",
        "        for c in range(env.W):\n",
        "            if (r,c) in env.walls:\n",
        "                row.append(\"■\")\n",
        "            elif (r,c) in env.terminals:\n",
        "                row.append(f\"T({env.terminals[(r,c)]:.1f})\")\n",
        "            elif (r,c) in policy and policy[(r,c)] is not None:\n",
        "                row.append(GridWorld.ARROWS[policy[(r,c)]])\n",
        "            else:\n",
        "                row.append(\"•\")\n",
        "        print(\" \".join(f\"{x:>6}\" for x in row))\n",
        "\n",
        "# -------------------------\n",
        "# TD(0) Policy Evaluation\n",
        "# -------------------------\n",
        "def td0_policy_evaluation(env, policy, gamma=0.99, alpha=0.1, episodes=500, max_steps=500):\n",
        "    V = defaultdict(float)\n",
        "    for _ in range(episodes):\n",
        "        s = env.reset()\n",
        "        for _ in range(max_steps):\n",
        "            if env.is_terminal(s): break\n",
        "            a = policy(s)\n",
        "            ns, r, done = env.step(a)\n",
        "            V[s] += alpha * (r + gamma * (0.0 if done else V[ns]) - V[s])\n",
        "            s = ns\n",
        "            if done: break\n",
        "    return V\n",
        "\n",
        "# -------------------------\n",
        "# SARSA Control\n",
        "# -------------------------\n",
        "def sarsa_control(env, gamma=0.99, alpha=0.1, epsilon=0.1, episodes=2000, max_steps=500, epsilon_decay=None, min_epsilon=0.01):\n",
        "    Q = defaultdict(float)\n",
        "    for ep in range(episodes):\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, s, env.legal_actions(s), epsilon)\n",
        "        for _ in range(max_steps):\n",
        "            ns, r, done = env.step(a)\n",
        "            na = None\n",
        "            if not done:\n",
        "                na = epsilon_greedy(Q, ns, env.legal_actions(ns), epsilon)\n",
        "                target = r + gamma * Q[(ns,na)]\n",
        "            else:\n",
        "                target = r\n",
        "            Q[(s,a)] += alpha * (target - Q[(s,a)])\n",
        "            s, a = ns, na\n",
        "            if done: break\n",
        "        if epsilon_decay:\n",
        "            epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "    policy = {}\n",
        "    for s in env.all_states():\n",
        "        if env.is_terminal(s):\n",
        "            policy[s] = None\n",
        "            continue\n",
        "        qs = [Q[(s,a)] for a in env.legal_actions(s)]\n",
        "        best = np.argmax(qs)\n",
        "        policy[s] = env.legal_actions(s)[best]\n",
        "    return Q, policy\n",
        "\n",
        "# -------------------------\n",
        "# Example Run\n",
        "# -------------------------\n",
        "H, W = 5, 5\n",
        "walls = {(1,1), (2,3)}\n",
        "terminals = {(0,4): +1.0, (4,0): -1.0}\n",
        "env = GridWorld(H=H, W=W, walls=walls, terminals=terminals, step_cost=-0.04, start=(4,4),\n",
        "                stochastic_slip=0.0, seed=42)\n",
        "\n",
        "# Fixed policy for TD(0) evaluation\n",
        "def fixed_policy(state):\n",
        "    if env.is_terminal(state): return 0\n",
        "    r,c = state\n",
        "    pref = [3,0,1,2]  # LEFT > UP > RIGHT > DOWN\n",
        "    for a in pref:\n",
        "        dr, dc = env.DIRS[a]\n",
        "        nr, nc = r+dr, c+dc\n",
        "        if env.in_bounds(nr,nc) and (nr,nc) not in env.walls:\n",
        "            return a\n",
        "    return 0\n",
        "\n",
        "# --- TD(0) ---\n",
        "V_td0 = td0_policy_evaluation(env, fixed_policy, gamma=0.99, alpha=0.15, episodes=800)\n",
        "print_policy(env, greedy_policy_from_V(env, V_td0), title=\"Greedy w.r.t. V (TD-0)\")\n",
        "plot_values(env, V_td0, title=\"TD(0) Value Function\")\n",
        "\n",
        "# --- SARSA ---\n",
        "Q_sarsa, pi_sarsa = sarsa_control(\n",
        "    env, gamma=0.99, alpha=0.15, epsilon=0.2,\n",
        "    episodes=4000, max_steps=200, epsilon_decay=0.9995, min_epsilon=0.02\n",
        ")\n",
        "print_policy(env, pi_sarsa, title=\"SARSA Learned Policy\")\n",
        "V_from_Q = defaultdict(float)\n",
        "for s in env.all_states():\n",
        "    if env.is_terminal(s):\n",
        "        V_from_Q[s] = 0.0\n",
        "    else:\n",
        "        V_from_Q[s] = max(Q_sarsa[(s,a)] for a in env.legal_actions(s))\n",
        "plot_values(env, V_from_Q, title=\"Value from SARSA Q\")"
      ]
    }
  ]
}